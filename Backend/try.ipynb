{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import io\n",
    "from google.cloud import storage\n",
    "from flask import Flask, request, render_template, redirect, url_for, send_from_directory\n",
    "from werkzeug.utils import secure_filename\n",
    "from datetime import datetime\n",
    "#from script import process_csv\n",
    "import google\n",
    "from flask import Flask\n",
    "from .Backend.cleaning import Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"C:\\Users\\arman\\Downloads\\automl-bigdata-7c2859c8477a.json\"\n",
    "def cloud_read(bucket_name, blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    data = blob.download_as_bytes()\n",
    "    df = pd.read_csv(io.BytesIO(data))\n",
    "    print(f'Pulled down file from bucket {bucket_name}, file name: {blob_name}')\n",
    "    return df\n",
    "    # with blob.open('r') as f:\n",
    "    #     csv_file = f.read()\n",
    "    # return csv_file\n",
    "\n",
    "def cloud_write(bucket_name, blob_name, csv_file):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(csv_file)\n",
    "#list all blobs in gcs\n",
    "\n",
    "def list_blobs(bucket_name):\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    storage_client = storage.Client()\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    # Note: The call returns a response only when the iterator is consumed.\n",
    "    ll = []\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('.csv'):\n",
    "            ll.append(blob.name)\n",
    "    return ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "ALLOWED_EXTENSIONS = set(['csv'])\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "def connection():\n",
    "  cred = credentials.Certificate(r\"C:\\Users\\arman\\Documents\\GitHub\\Auto-ML-BigDataArchitecture\\auto-ml-af39c-firebase-adminsdk-37cmd-35f3911f5e.json\")\n",
    "  try:\n",
    "    app = firebase_admin.initialize_app(cred)\n",
    "  except:\n",
    "    app = firebase_admin.initialize_app(cred, name = str(random.random()))\n",
    "  return firestore.client()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "@app.route('/uploading', methods=['GET', 'POST'])\n",
    "def uploading():\n",
    "    return render_template('upload.html')\n",
    "\n",
    "@app.route('/preprocess', methods=['GET', 'POST'])\n",
    "def preprocess():\n",
    "    csv_files = list_blobs('automl-bigdataarch')\n",
    "    csv_files = [i.split('/')[-1] for i in csv_files]\n",
    "    return render_template('preprocess.html', data = csv_files)\n",
    "\n",
    "@app.route('/preprocessResults', methods=['GET', 'POST'])\n",
    "def preprocessResults():\n",
    "    return render_template('preprocessResults.html')\n",
    "\n",
    "@app.route('/download/<path:filename>', methods=['GET'])\n",
    "def download(filename):\n",
    "    \"\"\"Download a file.\"\"\"\n",
    "    \n",
    "    full_path = os.path.join(app.root_path, r'C:\\Users\\arman\\Documents\\GitHub\\Auto-ML-BigDataArchitecture\\input')\n",
    "    \n",
    "    return send_from_directory(full_path, 'orange_vs_grapefruit_raw.csv', as_attachment=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/impute', methods=['GET', 'POST'])\n",
    "def impute():\n",
    "\n",
    "    # if request.method == 'POST':\n",
    "    filename = request.form.get(\"files\")\n",
    "    print(filename)\n",
    "    data = cloud_read('automl-bigdataarch', filename)\n",
    "    print(data.columns)\n",
    "    Target = data[['Target']]\n",
    "    data = data.drop('Target', axis = 1)\n",
    "\n",
    "    #data info\n",
    "    # Dashboard for raw data\n",
    "    num_cols_raw = [x for x in data.columns if data[x].dtype in ['int64', 'float64']]\n",
    "    numerical_cols_raw = len(num_cols_raw)\n",
    "    cat_cols_raw = [x for x in data.columns if data[x].dtype in ['object']]\n",
    "    categorical_columns_raw = len(cat_cols_raw)\n",
    "    num_columns_raw = [x for x in data.columns if data[x].dtype]\n",
    "    number_columns_raw = len(num_columns_raw)\n",
    "    num_rows_raw = len(data.index)\n",
    "\n",
    "    #checking missing values\n",
    "    percent_missing = data.isnull().sum() * 100 / data.shape[0]\n",
    "    #dropping columns if missing percentage is more than 30\n",
    "    for i in range(len(data.columns)):\n",
    "        if percent_missing[i] >30:\n",
    "            data.drop(data.columns[i],axis=1,inplace=True)\n",
    "    missing = [x for x in percent_missing if x > 0.0]\n",
    "    missing_rows_raw = len(missing)\n",
    "    #getting numerical and categorical variables\n",
    "    numerical_columns = [x for x in data.columns if data[x].dtype != 'object']\n",
    "    data_num = data[numerical_columns]\n",
    "    \n",
    "    cat_columns = [x for x in data.columns if x not in numerical_columns]\n",
    "    data_cat = data[cat_columns]\n",
    "    \n",
    "    #Imputing using KNN Imputer for numerical columns\n",
    "    imputer = KNNImputer(n_neighbors=2)\n",
    "    imputed_num = imputer.fit_transform(data_num)\n",
    "    imputed_num = pd.DataFrame(imputed_num)\n",
    "    imputed_num.columns=data_num.columns\n",
    "    \n",
    "    # most frequent imputation for categorical columns\n",
    "    data_cat_imputed = data_cat.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
    "    \n",
    "    #concat the imputed dfs\n",
    "    imputed_data = pd.concat([imputed_num, data_cat_imputed, Target], axis=1)\n",
    "\n",
    "    # Dashboard for imputed data\n",
    "    num_cols_imp = [x for x in imputed_data.columns if imputed_data[x].dtype in ['int64', 'float64']]\n",
    "    numerical_cols_imp = len(num_cols_imp)\n",
    "    cat_cols_imp = [x for x in imputed_data.columns if imputed_data[x].dtype in ['object']]\n",
    "    categorical_columns_imp = len(cat_cols_imp)\n",
    "    num_columns_imp = [x for x in imputed_data.columns if imputed_data[x].dtype]\n",
    "    number_columns_imp = len(num_columns_imp)\n",
    "    num_rows_imp = len(imputed_data.index)\n",
    "    missing_imp = [x for x in percent_missing if x > 0.30]\n",
    "    missing_rows_imp = len(missing_imp)\n",
    "    \n",
    "    frontend_data = {\n",
    "        'Raw_numericalvalues': numerical_cols_raw,\n",
    "        'Raw_categoricalvalues': categorical_columns_raw,\n",
    "        'Raw_columns': number_columns_raw,\n",
    "        'Raw_rows': num_rows_raw,\n",
    "        'Raw_missing': missing_rows_raw,\n",
    "        'Imputed_numericalvalues': numerical_cols_imp,\n",
    "        'Imputed_categoricalvalues': categorical_columns_imp,\n",
    "        'Imputed_columns': number_columns_imp,\n",
    "        'Imputed_rows': num_rows_imp,\n",
    "        'Imputed_missingvalues': missing_rows_imp\n",
    "      }\n",
    "    imputed_data = normalize_and_encode(imputed_data)\n",
    "    \n",
    "\n",
    "    imputed_data.to_csv('upload_imputed.csv',index = False)\n",
    "    cloud_write('automl-bigdataarch', f'{filename.split(\".\")[0]}_preprocessed.csv','upload_imputed.csv')\n",
    "    return render_template('preprocessResults.html', data = frontend_data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
