{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "xn8O8RgLooTN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import string\n",
        "import random\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_df = pd.read_csv(r\"test.csv\")"
      ],
      "metadata": {
        "id": "GOjWs7HN8fpc"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/maximal-record-384001-406302dda581.json' "
      ],
      "metadata": {
        "id": "keYZ03Nt67ay"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "def write_read(bucket_name, blob_name):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename('/content/Dummy Data HSS.csv')"
      ],
      "metadata": {
        "id": "81l1tpvl7D2G"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "def write_read(bucket_name, blob_name):\n",
        "    \"\"\"Write and read a blob from GCS using file-like IO\"\"\"\n",
        "    # The ID of your GCS bucket\n",
        "    # bucket_name = \"your-bucket-name\"\n",
        "\n",
        "    # The ID of your new GCS object\n",
        "    # blob_name = \"storage-object-name\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "\n",
        "    # Mode can be specified as wb/rb for bytes mode.\n",
        "    # See: https://docs.python.org/3/library/io.html\n",
        "    with blob.open(\"w\") as f:\n",
        "        f.write(\"Hello world\")\n",
        "\n",
        "    with blob.open(\"r\") as f:\n",
        "        print(f.read())\n"
      ],
      "metadata": {
        "id": "pgY4ddyx7F0z"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute(data):\n",
        "    #checking missing values\n",
        "    percent_missing = data.isnull().sum() * 100 / data.shape[0]\n",
        "    #dropping columns if missing percentage is more than 30\n",
        "    for i in range(len(data.columns)):\n",
        "        if percent_missing[i] >30:\n",
        "            data.drop(data.columns[i],axis=1,inplace=True)\n",
        "    #getting numerical and categorical variables\n",
        "    numerical_columns = [x for x in data.columns if data[x].dtype != 'object']\n",
        "    data_num = data[numerical_columns]\n",
        "    \n",
        "    cat_columns = [x for x in data.columns if x not in numerical_columns]\n",
        "    data_cat = data[cat_columns]\n",
        "    \n",
        "    #Imputing using KNN Imputer for numerical columns\n",
        "    imputer = KNNImputer(n_neighbors=2)\n",
        "    imputed_num = imputer.fit_transform(data_num)\n",
        "    imputed_num = pd.DataFrame(imputed_num)\n",
        "    imputed_num.columns=data_num.columns\n",
        "    \n",
        "    # most frequent imputation for categorical columns\n",
        "    data_cat_imputed = data_cat.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
        "    \n",
        "    #concat the imputed dfs\n",
        "    imputed_data = pd.concat([imputed_num, data_cat_imputed], axis=1)\n",
        "    \n",
        "    #return imputed_data\n",
        "    return imputed_data"
      ],
      "metadata": {
        "id": "Qaky0CIK7IB2"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ll = impute(demo_df)"
      ],
      "metadata": {
        "id": "i7Aq1nm37KBo"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_encode(imputed_data):\n",
        "    #normalizing numerical columns using robustscalar\n",
        "    numerical_columns  = [x for x in imputed_data.columns if imputed_data[x].dtype in ['int64', 'float64']]\n",
        "    scalar = RobustScaler(quantile_range=(25,75))\n",
        "    scaled = scalar.fit_transform(imputed_data[numerical_columns])\n",
        "    scaled = pd.DataFrame(scaled)\n",
        "    scaled.columns = imputed_data[numerical_columns].columns\n",
        "    \n",
        "    #dropping cat columns with more than 10 categories\n",
        "    cat_cols = [x for x in imputed_data.columns if x not in numerical_columns]\n",
        "    cat_cols_to_drop = []\n",
        "    for col in cat_cols:\n",
        "        if imputed_data[col].value_counts().count()>10:\n",
        "            cat_cols_to_drop.append(col)\n",
        "    data_for_enc = imputed_data.drop(numerical_columns,axis=1)\n",
        "    data_for_enc.drop(cat_cols_to_drop,axis=1,inplace=True)\n",
        "\n",
        "    #encoding categorical varialbles\n",
        "    try:\n",
        "        enc_data= pd.get_dummies(data_for_enc, columns=data_for_enc.columns)\n",
        "        encoded_data = pd.concat([scaled, enc_data], axis=1)\n",
        "    except:\n",
        "      encoded_data = scaled.copy()\n",
        "\n",
        "    return encoded_data"
      ],
      "metadata": {
        "id": "21FLfGo37Lxa"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = normalize_and_encode(ll)"
      ],
      "metadata": {
        "id": "6hZ8Q_8W7NoZ"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'/content/maximal-record-384001-406302dda581.json' \n",
        "def cloud_read(bucket_name, blob_name):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    pickle_in = blob.download_as_string()\n",
        "    my_dictionary = pickle.loads(pickle_in)\n",
        "    #pickled_model = pickle.load(open(data, 'rb'))\n",
        "    print(f'Pulled down file from bucket {bucket_name}, file name: {blob_name}')\n",
        "    return my_dictionary"
      ],
      "metadata": {
        "id": "-4G9dI3NUxuA"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickled_model=cloud_read('automl-bigdata','model.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjFTtYSJVHgB",
        "outputId": "44c7570b-39e4-42a1-c64e-10b1824a524f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pulled down file from bucket automl-bigdata, file name: model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_file = pickled_model.predict(test)\n",
        "pd.DataFrame({\"Predictions\":pred_file}).to_csv(\"pred_file.csv\",index=False)"
      ],
      "metadata": {
        "id": "faaizPE87YOW"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cloud_pred(bucket_name, blob_name, pred_file):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename(pred_file)"
      ],
      "metadata": {
        "id": "TBaBGuV_bvxP"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cloud_pred('automl-bigdata', 'pred_file', 'pred_file.csv')"
      ],
      "metadata": {
        "id": "Lg6vYWOhb59t"
      },
      "execution_count": 95,
      "outputs": []
    }
  ]
}