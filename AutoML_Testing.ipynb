{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "xn8O8RgLooTN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import string\n",
        "import random\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_df = pd.read_csv(r\"test.csv\")"
      ],
      "metadata": {
        "id": "GOjWs7HN8fpc"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/maximal-record-384001-406302dda581.json' "
      ],
      "metadata": {
        "id": "keYZ03Nt67ay"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "def write_read(bucket_name, blob_name):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename('/content/Dummy Data HSS.csv')"
      ],
      "metadata": {
        "id": "81l1tpvl7D2G"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "def write_read(bucket_name, blob_name):\n",
        "    \"\"\"Write and read a blob from GCS using file-like IO\"\"\"\n",
        "    # The ID of your GCS bucket\n",
        "    # bucket_name = \"your-bucket-name\"\n",
        "\n",
        "    # The ID of your new GCS object\n",
        "    # blob_name = \"storage-object-name\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "\n",
        "    # Mode can be specified as wb/rb for bytes mode.\n",
        "    # See: https://docs.python.org/3/library/io.html\n",
        "    with blob.open(\"w\") as f:\n",
        "        f.write(\"Hello world\")\n",
        "\n",
        "    with blob.open(\"r\") as f:\n",
        "        print(f.read())\n"
      ],
      "metadata": {
        "id": "pgY4ddyx7F0z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute(data):\n",
        "    #checking missing values\n",
        "    percent_missing = data.isnull().sum() * 100 / data.shape[0]\n",
        "    #dropping columns if missing percentage is more than 30\n",
        "    for i in range(len(data.columns)):\n",
        "        if percent_missing[i] >30:\n",
        "            data.drop(data.columns[i],axis=1,inplace=True)\n",
        "    #getting numerical and categorical variables\n",
        "    numerical_columns = [x for x in data.columns if data[x].dtype != 'object']\n",
        "    data_num = data[numerical_columns]\n",
        "    \n",
        "    cat_columns = [x for x in data.columns if x not in numerical_columns]\n",
        "    data_cat = data[cat_columns]\n",
        "    \n",
        "    #Imputing using KNN Imputer for numerical columns\n",
        "    imputer = KNNImputer(n_neighbors=2)\n",
        "    imputed_num = imputer.fit_transform(data_num)\n",
        "    imputed_num = pd.DataFrame(imputed_num)\n",
        "    imputed_num.columns=data_num.columns\n",
        "    \n",
        "    # most frequent imputation for categorical columns\n",
        "    data_cat_imputed = data_cat.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
        "    \n",
        "    #concat the imputed dfs\n",
        "    imputed_data = pd.concat([imputed_num, data_cat_imputed], axis=1)\n",
        "    \n",
        "    #return imputed_data\n",
        "    return imputed_data"
      ],
      "metadata": {
        "id": "Qaky0CIK7IB2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ll = impute(demo_df)"
      ],
      "metadata": {
        "id": "i7Aq1nm37KBo"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_encode(imputed_data):\n",
        "    #normalizing numerical columns using robustscalar\n",
        "    numerical_columns  = [x for x in imputed_data.columns if imputed_data[x].dtype in ['int64', 'float64']]\n",
        "    scalar = RobustScaler(quantile_range=(25,75))\n",
        "    scaled = scalar.fit_transform(imputed_data[numerical_columns])\n",
        "    scaled = pd.DataFrame(scaled)\n",
        "    scaled.columns = imputed_data[numerical_columns].columns\n",
        "    \n",
        "    #dropping cat columns with more than 10 categories\n",
        "    cat_cols = [x for x in imputed_data.columns if x not in numerical_columns]\n",
        "    cat_cols_to_drop = []\n",
        "    for col in cat_cols:\n",
        "        if imputed_data[col].value_counts().count()>10:\n",
        "            cat_cols_to_drop.append(col)\n",
        "    data_for_enc = imputed_data.drop(numerical_columns,axis=1)\n",
        "    data_for_enc.drop(cat_cols_to_drop,axis=1,inplace=True)\n",
        "\n",
        "    #encoding categorical varialbles\n",
        "    try:\n",
        "        enc_data= pd.get_dummies(data_for_enc, columns=data_for_enc.columns)\n",
        "        encoded_data = pd.concat([scaled, enc_data], axis=1)\n",
        "    except:\n",
        "      encoded_data = scaled.copy()\n",
        "\n",
        "    return encoded_data"
      ],
      "metadata": {
        "id": "21FLfGo37Lxa"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = normalize_and_encode(ll)"
      ],
      "metadata": {
        "id": "6hZ8Q_8W7NoZ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'/content/maximal-record-384001-406302dda581.json' \n",
        "def cloud_read(bucket_name, blob_name):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    pickle_in = blob.download_as_string()\n",
        "    my_dictionary = pickle.loads(pickle_in)\n",
        "    #pickled_model = pickle.load(open(data, 'rb'))\n",
        "    print(f'Pulled down file from bucket {bucket_name}, file name: {blob_name}')\n",
        "    return my_dictionary"
      ],
      "metadata": {
        "id": "-4G9dI3NUxuA"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickled_model=cloud_read('automl-bigdata','model.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjFTtYSJVHgB",
        "outputId": "e40ac347-4404-49af-abd7-11f226ceb022"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pulled down file from bucket automl-bigdata, file name: model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickled_model.predict(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faaizPE87YOW",
        "outputId": "ff94c1ba-ccb6-4fba-e59d-78db73b2ab3c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.61213965, -0.27344059, -0.57089536, ..., -0.39945466,\n",
              "        0.31749858, -0.27763015])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ]
}