{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "UHXIXZWD8q0n"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import string\n",
        "import random\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "import pickle\n",
        "from flask import Flask\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
        "from scipy import stats\n",
        "import plotly.express as px\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_df = pd.read_csv(r\"train.csv\")"
      ],
      "metadata": {
        "id": "73lSYjzI-LsZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/automl-bigdata-7c2859c8477a.json' "
      ],
      "metadata": {
        "id": "T6MGDyX09L5V"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "def write_read(bucket_name, blob_name):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename('/content/Dummy Data HSS.csv')"
      ],
      "metadata": {
        "id": "FdqPX3pj8qNf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute(data):\n",
        "    # Dashboard for raw data\n",
        "    num_cols_raw = [x for x in data.columns if data[x].dtype in ['int64', 'float64']]\n",
        "    numerical_cols_raw = len(num_cols_raw)\n",
        "    cat_cols_raw = [x for x in data.columns if data[x].dtype in ['object']]\n",
        "    categorical_columns_raw = len(cat_cols_raw)\n",
        "    num_columns_raw = [x for x in data.columns if data[x].dtype]\n",
        "    number_columns_raw = len(num_columns_raw)\n",
        "    num_rows_raw = len(data.index)\n",
        "\n",
        "    #checking missing values\n",
        "    percent_missing = data.isnull().sum() * 100 / data.shape[0]\n",
        "    #dropping columns if missing percentage is more than 30\n",
        "    for i in range(len(data.columns)):\n",
        "        if percent_missing[i] >30:\n",
        "            data.drop(data.columns[i],axis=1,inplace=True)\n",
        "    \n",
        "    missing = [x for x in percent_missing if x > 0.0]\n",
        "    missing_rows_raw = len(missing)\n",
        "\n",
        "    rawdata_dash = {\n",
        "        'Raw_numericalvalues': numerical_cols_raw,\n",
        "        'Raw_categoricalvalues': categorical_columns_raw,\n",
        "        'Raw_columns': number_columns_raw,\n",
        "        'Raw_rows': num_rows_raw,\n",
        "        'Raw_missing': missing_rows_raw\n",
        "      } \n",
        "\n",
        "    #getting numerical and categorical variables\n",
        "    numerical_columns = [x for x in data.columns if data[x].dtype != 'object']\n",
        "    data_num = data[numerical_columns]\n",
        "    \n",
        "    cat_columns = [x for x in data.columns if x not in numerical_columns]\n",
        "    data_cat = data[cat_columns]\n",
        "    \n",
        "    #Imputing using KNN Imputer for numerical columns\n",
        "    imputer = KNNImputer(n_neighbors=2)\n",
        "    imputed_num = imputer.fit_transform(data_num)\n",
        "    imputed_num = pd.DataFrame(imputed_num)\n",
        "    imputed_num.columns=data_num.columns\n",
        "    \n",
        "    # most frequent imputation for categorical columns\n",
        "    data_cat_imputed = data_cat.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
        "    \n",
        "    #concat the imputed dfs\n",
        "    imputed_data = pd.concat([imputed_num, data_cat_imputed], axis=1)\n",
        "\n",
        "    # Dashboard for imputed data\n",
        "\n",
        "    num_cols_imp = [x for x in imputed_data.columns if imputed_data[x].dtype in ['int64', 'float64']]\n",
        "    numerical_cols_imp = len(num_cols_imp)\n",
        "    cat_cols_imp = [x for x in imputed_data.columns if imputed_data[x].dtype in ['object']]\n",
        "    categorical_columns_imp = len(cat_cols_imp)\n",
        "    num_columns_imp = [x for x in imputed_data.columns if imputed_data[x].dtype]\n",
        "    number_columns_imp = len(num_columns_imp)\n",
        "    num_rows_imp = len(imputed_data.index)\n",
        "\n",
        "    missing_imp = [x for x in percent_missing if x > 0.30]\n",
        "    missing_rows_imp = len(missing_imp)\n",
        "    \n",
        "    impdata_dash = {\n",
        "        'Imputed_numericalvalues': numerical_cols_imp,\n",
        "        'Imputed_categoricalvalues': categorical_columns_imp,\n",
        "        'Imputed_columns': number_columns_imp,\n",
        "        'Imputed_rows': num_rows_imp,\n",
        "        'Imputed_missingvalues': missing_rows_imp\n",
        "      } \n",
        "\n",
        "    #return imputed_data\n",
        "    return imputed_data,rawdata_dash, impdata_dash"
      ],
      "metadata": {
        "id": "Gf30A0ccsoXa"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ll = impute(demo_df)\n",
        "ll"
      ],
      "metadata": {
        "id": "8nu5aEZSspik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87eacb8d-5091-4465-93e5-aa747c4d2320"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(     Unnamed: 0  PassengerId  Survived  Sex     Age      Fare  Pclass_1  \\\n",
              " 0           0.0          1.0       0.0  1.0  0.2750  0.014151       0.0   \n",
              " 1           1.0          2.0       1.0  0.0  0.4750  0.139136       1.0   \n",
              " 2           2.0          3.0       1.0  0.0  0.3250  0.015469       0.0   \n",
              " 3           3.0          4.0       1.0  0.0  0.4375  0.103644       1.0   \n",
              " 4           4.0          5.0       0.0  1.0  0.4375  0.015713       0.0   \n",
              " ..          ...          ...       ...  ...     ...       ...       ...   \n",
              " 787       787.0        788.0       0.0  1.0  0.1000  0.056848       0.0   \n",
              " 788       788.0        789.0       1.0  1.0  0.0125  0.040160       0.0   \n",
              " 789       789.0        790.0       0.0  1.0  0.5750  0.154588       1.0   \n",
              " 790       790.0        791.0       0.0  1.0  0.3500  0.015127       0.0   \n",
              " 791       791.0        792.0       0.0  1.0  0.2000  0.050749       0.0   \n",
              " \n",
              "      Pclass_2  Pclass_3  Family_size  Title_1  Title_2  Title_3  Title_4  \\\n",
              " 0         0.0       1.0          0.1      1.0      0.0      0.0      0.0   \n",
              " 1         0.0       0.0          0.1      1.0      0.0      0.0      0.0   \n",
              " 2         0.0       1.0          0.0      0.0      0.0      0.0      1.0   \n",
              " 3         0.0       0.0          0.1      1.0      0.0      0.0      0.0   \n",
              " 4         0.0       1.0          0.0      1.0      0.0      0.0      0.0   \n",
              " ..        ...       ...          ...      ...      ...      ...      ...   \n",
              " 787       0.0       1.0          0.5      0.0      0.0      1.0      0.0   \n",
              " 788       0.0       1.0          0.3      0.0      0.0      1.0      0.0   \n",
              " 789       0.0       0.0          0.0      1.0      0.0      0.0      0.0   \n",
              " 790       0.0       1.0          0.0      1.0      0.0      0.0      0.0   \n",
              " 791       1.0       0.0          0.0      1.0      0.0      0.0      0.0   \n",
              " \n",
              "      Emb_1  Emb_2  Emb_3  \n",
              " 0      0.0    0.0    1.0  \n",
              " 1      1.0    0.0    0.0  \n",
              " 2      0.0    0.0    1.0  \n",
              " 3      0.0    0.0    1.0  \n",
              " 4      0.0    0.0    1.0  \n",
              " ..     ...    ...    ...  \n",
              " 787    0.0    1.0    0.0  \n",
              " 788    0.0    0.0    1.0  \n",
              " 789    1.0    0.0    0.0  \n",
              " 790    0.0    1.0    0.0  \n",
              " 791    0.0    0.0    1.0  \n",
              " \n",
              " [792 rows x 17 columns],\n",
              " {'Raw_numericalvalues': 17,\n",
              "  'Raw_categoricalvalues': 0,\n",
              "  'Raw_columns': 17,\n",
              "  'Raw_rows': 792,\n",
              "  'Raw_missing': 0},\n",
              " {'Imputed_numericalvalues': 17,\n",
              "  'Imputed_categoricalvalues': 0,\n",
              "  'Imputed_columns': 17,\n",
              "  'Imputed_rows': 792,\n",
              "  'Imputed_missingvalues': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_encode(imputed_data):\n",
        "    #normalizing numerical columns using robustscalar\n",
        "    numerical_columns  = [x for x in imputed_data.columns if imputed_data[x].dtype in ['int64', 'float64']]\n",
        "    scalar = RobustScaler(quantile_range=(25,75))\n",
        "    scaled = scalar.fit_transform(imputed_data[numerical_columns])\n",
        "    scaled = pd.DataFrame(scaled)\n",
        "    scaled.columns = imputed_data[numerical_columns].columns\n",
        "    \n",
        "    #dropping cat columns with more than 10 categories\n",
        "    cat_cols = [x for x in imputed_data.columns if x not in numerical_columns]\n",
        "    cat_cols_to_drop = []\n",
        "    for col in cat_cols:\n",
        "        if imputed_data[col].value_counts().count()>10:\n",
        "            cat_cols_to_drop.append(col)\n",
        "    data_for_enc = imputed_data.drop(numerical_columns,axis=1)\n",
        "    data_for_enc.drop(cat_cols_to_drop,axis=1,inplace=True)\n",
        "\n",
        "    #encoding categorical varialbles\n",
        "    try:\n",
        "        enc_data= pd.get_dummies(data_for_enc, columns=data_for_enc.columns)\n",
        "        encoded_data = pd.concat([scaled, enc_data], axis=1)\n",
        "    except:\n",
        "      encoded_data = scaled.copy()\n",
        "\n",
        "    return encoded_data"
      ],
      "metadata": {
        "id": "dNEQcU_gtEMt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = normalize_and_encode(ll[0])"
      ],
      "metadata": {
        "id": "JPk6JpBytHzU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [\n",
        "    XGBClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    GradientBoostingClassifier(),\n",
        "    LogisticRegression(),\n",
        "    DecisionTreeClassifier()\n",
        "    ]"
      ],
      "metadata": {
        "id": "lHBs8gDwZTqs"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_models = [\n",
        "    KNeighborsRegressor(),\n",
        "    LinearRegression(),\n",
        "    GradientBoostingRegressor(),\n",
        "    ExtraTreesRegressor(),\n",
        "    RandomForestRegressor(),\n",
        "    DecisionTreeRegressor(),\n",
        "    Lasso(),\n",
        "    Ridge()\n",
        "]"
      ],
      "metadata": {
        "id": "qtxnpR4jwZJB"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'/content/automl-bigdata-514a5baac622.json' \n",
        "def cloud_access(bucket_name, blob_name, pickle_file):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename(pickle_file)"
      ],
      "metadata": {
        "id": "S2dsziEgPMds"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def connection():\n",
        "  cred = credentials.Certificate('/content/auto-ml-af39c-firebase-adminsdk-37cmd-35f3911f5e.json')\n",
        "  try:\n",
        "    app = firebase_admin.initialize_app(cred)\n",
        "  except:\n",
        "    app = firebase_admin.initialize_app(cred, name = str(random.random()))\n",
        "  return firestore.client()\n",
        "\n",
        "def regression(train_data, y, reg_models):\n",
        "  db = connection()  \n",
        "  y_class = train_data[[y]]\n",
        "  \n",
        "  X_train, X_val, y_train, y_val = train_test_split(train_data.drop(y, axis=1), y_class, test_size=0.2, random_state=100)\n",
        "  \n",
        "  res = {}\n",
        "  \n",
        "  KNeighborsRegressor_grid = {\n",
        "      'n_neighbors':[2,5,10], \n",
        "      'weights': ['uniform', 'distance'], \n",
        "      'algorithm': ['auto','ball_tree','kd_tree','brute'],\n",
        "      'leaf_size': [15,30,45],\n",
        "      }\n",
        "\n",
        "  GradientBoostingRegressor_grid = {\n",
        "      'loss':['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
        "      'learning_rate':[0.1,0.5,0.8],\n",
        "      'n_estimators':[10,50,100]\n",
        "  }\n",
        "\n",
        "  ExtraTreesRegressor_grid = {\n",
        "      'n_estimators':[10,50,100],\n",
        "      'criterion':['squared_error', 'absolute_error', 'friedman_mse', 'poisson']\n",
        "  }\n",
        "\n",
        "  RandomForestRegressor_grid = {\n",
        "      'n_estimators':[10,50,100],\n",
        "      'criterion':['squared_error', 'absolute_error', 'friedman_mse', 'poisson']\n",
        "  }\n",
        "\n",
        "  DecisionTreeRegressor_grid = {\n",
        "      'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
        "      'splitter':['best','random']\n",
        "  }\n",
        "\n",
        "  LinearRegression_grid = {\n",
        "    'fit_intercept': [True, False]\n",
        "  }\n",
        "\n",
        "  Lasso_grid = {\n",
        "      'alpha': [0.1, 0.2, 0.5],\n",
        "      'fit_intercept': [True, False]\n",
        "  }\n",
        "  Ridge_grid = {\n",
        "       'alpha': [0.1, 0.2, 0.5],\n",
        "      'fit_intercept': [True, False]\n",
        "  }\n",
        "  \n",
        " \n",
        "  params = { \n",
        "      'KNeighborsRegressor': KNeighborsRegressor_grid,\n",
        "      'GradientBoostingRegressor': GradientBoostingRegressor_grid,\n",
        "      'ExtraTreesRegressor': ExtraTreesRegressor_grid,\n",
        "      'RandomForestRegressor': RandomForestRegressor_grid,\n",
        "      'DecisionTreeRegressor': DecisionTreeRegressor_grid,\n",
        "      'LinearRegression': LinearRegression_grid, \n",
        "      'Lasso': Lasso_grid,\n",
        "      'Ridge':Ridge_grid\n",
        "    }\n",
        "\n",
        "  clf = {}\n",
        "\n",
        "  for reg in reg_models:\n",
        "    name = reg.__class__.__name__  \n",
        "    try:\n",
        "      clf[name] = RandomizedSearchCV(reg, params[name], random_state=0)\n",
        "    except:\n",
        "      print(name)\n",
        "      continue\n",
        "    results = clf[name].fit(X_train, y_train)\n",
        "    print(results.best_params_)\n",
        "    r2 = round(r2_score(y_val, clf[name].predict(X_val)), 3)\n",
        "    rmse = round(mean_squared_error(y_val, clf[name].predict(X_val)), 3)\n",
        "    N = 16\n",
        " \n",
        "    string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    while string_name in db.collection(u'models').stream():\n",
        "        string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    print(\"{} trained with an RMSE of : {} and an accuracy of: {}\".format(name, rmse, r2))\n",
        "    \n",
        "    res[name] = {\n",
        "        'RMSE': rmse,\n",
        "         'r2': r2,\n",
        "         'params': results.best_params_\n",
        "      }  \n",
        "\n",
        "  rmse_list = []\n",
        "  r2_list = []\n",
        "  names = list(res.keys())\n",
        "  for name in res:\n",
        "    rmse_list.append(res[name]['RMSE'])\n",
        "    r2_list.append(res[name]['r2'])\n",
        "\n",
        "  if rmse_list.count(min(rmse_list)) > 1:\n",
        "    best_model = names[r2_list.index(max(r2_list))]\n",
        "  else:\n",
        "    best_model = names[rmse_list.index(min(rmse_list))]\n",
        "\n",
        "  print(best_model, clf[best_model].get_params())\n",
        "  pickle.dump(clf[best_model], open('model.pkl', 'wb'))\n",
        "  cloud_access('automl-bigdataarch', 'regression_models/model.pkl', 'model.pkl')\n",
        "  db.collection(u'models').document(string_name).set(res)\n",
        "  return best_model, res"
      ],
      "metadata": {
        "id": "vbFeRRK_T9zU"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def connection():\n",
        "  cred = credentials.Certificate('/content/auto-ml-af39c-firebase-adminsdk-37cmd-35f3911f5e.json')\n",
        "  try:\n",
        "    app = firebase_admin.initialize_app(cred)\n",
        "  except:\n",
        "    app = firebase_admin.initialize_app(cred, name = str(random.random()))\n",
        "  return firestore.client()\n",
        "\n",
        "def classification(train_data, y, classifiers):\n",
        "  db = connection()  \n",
        "  y_class = train_data[[y]]\n",
        "  X_train, X_val, y_train, y_val = train_test_split(train_data.drop(y, axis=1), y_class, test_size=0.2, random_state=100)\n",
        "\n",
        "  res = {}\n",
        "  \n",
        "  XGBClassifier_grid = {\n",
        "      'n_estimators': stats.randint(50, 100),\n",
        "      'learning_rate': stats.uniform(0.01, 0.59),\n",
        "      'subsample': stats.uniform(0.3, 0.6),\n",
        "      'max_depth': [3, 4, 5],\n",
        "      'colsample_bytree': stats.uniform(0.5, 0.4),\n",
        "      'min_child_weight': [1, 2, 3, 4]\n",
        "      }\n",
        "\n",
        "  RandomForestClassifier_grid = {\n",
        "      'n_estimators':[10,50,100],\n",
        "      'criterion':['gini', 'entropy', 'log_loss']\n",
        "  }\n",
        "\n",
        "  GradientBoostingClassifier_grid = {\n",
        "      'loss':['log_loss', 'deviance', 'exponential'],\n",
        "      'learning_rate':[0.1,0.5]\n",
        "        }\n",
        "\n",
        "  LogisticRegression_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'dual':[True, False],\n",
        "    'fit_intercept':[True,False]\n",
        "  }\n",
        "\n",
        "  DecisionTreeClassifier_grid = {\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "    'splitter':['best', 'random']\n",
        "  }\n",
        "  \n",
        "  params = { \n",
        "      'XGBClassifier': XGBClassifier_grid,\n",
        "      'RandomForestClassifier': RandomForestClassifier_grid,\n",
        "      'GradientBoostingClassifier': GradientBoostingClassifier_grid,\n",
        "      'LogisticRegression': LogisticRegression_grid,\n",
        "      'DecisionTreeClassifier':DecisionTreeClassifier_grid\n",
        "    }\n",
        "    \n",
        "  clf = {}\n",
        "  \n",
        "  for clf1 in classifiers:\n",
        "    name = clf1.__class__.__name__\n",
        "    try:\n",
        "      clf[name] = RandomizedSearchCV(clf1, params[name], random_state=0)\n",
        "    except:\n",
        "        print(name)\n",
        "        continue \n",
        "\n",
        "    results = clf[name].fit(X_train, y_train)\n",
        "    print(results.best_params_)        \n",
        "    acc = round(balanced_accuracy_score(y_val, clf[name].predict(X_val)), 3)\n",
        "    f1 = round(f1_score(y_true=y_val, y_pred = clf[name].predict(X_val), average='weighted'), 3)\n",
        "\n",
        "    N = 16\n",
        " \n",
        "    string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    while string_name in db.collection(u'models').stream():\n",
        "        string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    print(\"{} trained with an F1 of : {} and an accuracy of: {}\".format(name, f1, acc))\n",
        "\n",
        "    res[name] = {\n",
        "        'Accuracy': acc,\n",
        "         'F1Score': f1,\n",
        "         'params': results.best_params_\n",
        "      }  \n",
        "\n",
        "  acc_list = []\n",
        "  f1_list = []\n",
        "  names = list(res.keys())\n",
        "  for name in res:\n",
        "    acc_list.append(res[name]['Accuracy'])\n",
        "    f1_list.append(res[name]['F1Score'])\n",
        "\n",
        "  if acc_list.count(max(acc_list)) > 1:\n",
        "    best_model = names[f1_list.index(max(f1_list))]\n",
        "  else:\n",
        "    best_model = names[acc_list.index(max(acc_list))]\n",
        "\n",
        "  print(best_model, clf[best_model].get_params())\n",
        "  pickle.dump(clf[best_model], open('model.pkl', 'wb'))\n",
        "  cloud_access('automl-bigdataarch', 'classification_models/model.pkl', 'model.pkl')\n",
        "  db.collection(u'models').document(string_name).set(res)\n",
        "  return best_model, res"
      ],
      "metadata": {
        "id": "voDTtw_pZ3XZ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M9MzjE8q4eic"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression(train,'cnt',reg_models)"
      ],
      "metadata": {
        "id": "EDNRfUKvf5uM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "bf774d08-d9be-4fc8-ab30-a3d018b064b3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-817cc2bdf14d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mregression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cnt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-50-2de33690d7e5>\u001b[0m in \u001b[0;36mregression\u001b[0;34m(train_data, y, reg_models)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0my_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3813\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3815\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6068\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6070\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6072\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6129\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6130\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6132\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['cnt'], dtype='object')] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification(train,'Survived',classifiers)"
      ],
      "metadata": {
        "id": "l7xpgUV9brG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba9331a-f303-4dcb-868a-104bbcdb44a6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'colsample_bytree': 0.7470541988303508, 'learning_rate': 0.3711364764062286, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 65, 'subsample': 0.8415590853176427}\n",
            "XGBClassifier trained with an F1 of : 0.751 and an accuracy of: 0.728\n",
            "{'n_estimators': 100, 'criterion': 'gini'}\n",
            "RandomForestClassifier trained with an F1 of : 0.802 and an accuracy of: 0.781\n",
            "{'loss': 'deviance', 'learning_rate': 0.1}\n",
            "GradientBoostingClassifier trained with an F1 of : 0.791 and an accuracy of: 0.774\n",
            "{'penalty': 'l2', 'fit_intercept': True, 'dual': False}\n",
            "LogisticRegression trained with an F1 of : 0.79 and an accuracy of: 0.768\n",
            "{'splitter': 'best', 'criterion': 'log_loss'}\n",
            "DecisionTreeClassifier trained with an F1 of : 0.732 and an accuracy of: 0.724\n",
            "RandomForestClassifier {'cv': None, 'error_score': nan, 'estimator__bootstrap': True, 'estimator__ccp_alpha': 0.0, 'estimator__class_weight': None, 'estimator__criterion': 'gini', 'estimator__max_depth': None, 'estimator__max_features': 'sqrt', 'estimator__max_leaf_nodes': None, 'estimator__max_samples': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__n_estimators': 100, 'estimator__n_jobs': None, 'estimator__oob_score': False, 'estimator__random_state': None, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': RandomForestClassifier(), 'n_iter': 10, 'n_jobs': None, 'param_distributions': {'n_estimators': [10, 50, 100], 'criterion': ['gini', 'entropy', 'log_loss']}, 'pre_dispatch': '2*n_jobs', 'random_state': 0, 'refit': True, 'return_train_score': False, 'scoring': None, 'verbose': 0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('RandomForestClassifier',\n",
              " {'XGBClassifier': {'Accuracy': 0.728,\n",
              "   'F1Score': 0.751,\n",
              "   'params': {'colsample_bytree': 0.7470541988303508,\n",
              "    'learning_rate': 0.3711364764062286,\n",
              "    'max_depth': 3,\n",
              "    'min_child_weight': 2,\n",
              "    'n_estimators': 65,\n",
              "    'subsample': 0.8415590853176427}},\n",
              "  'RandomForestClassifier': {'Accuracy': 0.781,\n",
              "   'F1Score': 0.802,\n",
              "   'params': {'n_estimators': 100, 'criterion': 'gini'}},\n",
              "  'GradientBoostingClassifier': {'Accuracy': 0.774,\n",
              "   'F1Score': 0.791,\n",
              "   'params': {'loss': 'deviance', 'learning_rate': 0.1}},\n",
              "  'LogisticRegression': {'Accuracy': 0.768,\n",
              "   'F1Score': 0.79,\n",
              "   'params': {'penalty': 'l2', 'fit_intercept': True, 'dual': False}},\n",
              "  'DecisionTreeClassifier': {'Accuracy': 0.724,\n",
              "   'F1Score': 0.732,\n",
              "   'params': {'splitter': 'best', 'criterion': 'log_loss'}}})"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}