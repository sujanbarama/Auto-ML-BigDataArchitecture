{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UHXIXZWD8q0n"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import string\n",
        "import random\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "import pickle\n",
        "from flask import Flask\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
        "from scipy import stats\n",
        "import plotly.express as px\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_df = pd.read_csv(r\"train.csv\")"
      ],
      "metadata": {
        "id": "73lSYjzI-LsZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/automl-bigdata-7c2859c8477a.json' "
      ],
      "metadata": {
        "id": "T6MGDyX09L5V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "def write_read(bucket_name, blob_name):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename('/content/Dummy Data HSS.csv')"
      ],
      "metadata": {
        "id": "FdqPX3pj8qNf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute(data):\n",
        "    # Dashboard for raw data\n",
        "    num_cols_raw = [x for x in data.columns if data[x].dtype in ['int64', 'float64']]\n",
        "    numerical_cols_raw = len(num_cols_raw)\n",
        "    cat_cols_raw = [x for x in data.columns if data[x].dtype in ['object']]\n",
        "    categorical_columns_raw = len(cat_cols_raw)\n",
        "    num_columns_raw = [x for x in data.columns if data[x].dtype]\n",
        "    number_columns_raw = len(num_columns_raw)\n",
        "    num_rows_raw = len(data.index)\n",
        "\n",
        "    #checking missing values\n",
        "    percent_missing = data.isnull().sum() * 100 / data.shape[0]\n",
        "    #dropping columns if missing percentage is more than 30\n",
        "    for i in range(len(data.columns)):\n",
        "        if percent_missing[i] >30:\n",
        "            data.drop(data.columns[i],axis=1,inplace=True)\n",
        "    \n",
        "    missing = [x for x in percent_missing if x > 0.0]\n",
        "    missing_rows_raw = len(missing)\n",
        "\n",
        "    rawdata_dash = {\n",
        "        'Raw_numericalvalues': numerical_cols_raw,\n",
        "        'Raw_categoricalvalues': categorical_columns_raw,\n",
        "        'Raw_columns': number_columns_raw,\n",
        "        'Raw_rows': num_rows_raw,\n",
        "        'Raw_missing': missing_rows_raw\n",
        "      } \n",
        "\n",
        "    #getting numerical and categorical variables\n",
        "    numerical_columns = [x for x in data.columns if data[x].dtype != 'object']\n",
        "    data_num = data[numerical_columns]\n",
        "    \n",
        "    cat_columns = [x for x in data.columns if x not in numerical_columns]\n",
        "    data_cat = data[cat_columns]\n",
        "    \n",
        "    #Imputing using KNN Imputer for numerical columns\n",
        "    imputer = KNNImputer(n_neighbors=2)\n",
        "    imputed_num = imputer.fit_transform(data_num)\n",
        "    imputed_num = pd.DataFrame(imputed_num)\n",
        "    imputed_num.columns=data_num.columns\n",
        "    \n",
        "    # most frequent imputation for categorical columns\n",
        "    data_cat_imputed = data_cat.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
        "    \n",
        "    #concat the imputed dfs\n",
        "    imputed_data = pd.concat([imputed_num, data_cat_imputed], axis=1)\n",
        "\n",
        "    # Dashboard for imputed data\n",
        "\n",
        "    num_cols_imp = [x for x in imputed_data.columns if imputed_data[x].dtype in ['int64', 'float64']]\n",
        "    numerical_cols_imp = len(num_cols_imp)\n",
        "    cat_cols_imp = [x for x in imputed_data.columns if imputed_data[x].dtype in ['object']]\n",
        "    categorical_columns_imp = len(cat_cols_imp)\n",
        "    num_columns_imp = [x for x in imputed_data.columns if imputed_data[x].dtype]\n",
        "    number_columns_imp = len(num_columns_imp)\n",
        "    num_rows_imp = len(imputed_data.index)\n",
        "\n",
        "    missing_imp = [x for x in percent_missing if x > 0.30]\n",
        "    missing_rows_imp = len(missing_imp)\n",
        "    \n",
        "    impdata_dash = {\n",
        "        'Imputed_numericalvalues': numerical_cols_imp,\n",
        "        'Imputed_categoricalvalues': categorical_columns_imp,\n",
        "        'Imputed_columns': number_columns_imp,\n",
        "        'Imputed_rows': num_rows_imp,\n",
        "        'Imputed_missingvalues': missing_rows_imp\n",
        "      } \n",
        "\n",
        "    #return imputed_data\n",
        "    return imputed_data,rawdata_dash, impdata_dash"
      ],
      "metadata": {
        "id": "Gf30A0ccsoXa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ll = impute(demo_df)\n",
        "ll"
      ],
      "metadata": {
        "id": "8nu5aEZSspik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500925c2-9e67-45ac-b178-1f1658cb23d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(            ID    hr  weathersit  temp   atemp   hum  windspeed  casual  \\\n",
              " 0       3094.0  20.0         2.0  0.52  0.5000  0.83     0.1343    22.0   \n",
              " 1      10645.0  16.0         3.0  0.50  0.4848  0.94     0.2985    49.0   \n",
              " 2       2114.0  23.0         1.0  0.32  0.3182  0.61     0.1642    20.0   \n",
              " 3      15289.0   6.0         3.0  0.62  0.5455  0.94     0.0896     1.0   \n",
              " 4       2273.0  14.0         2.0  0.36  0.3485  0.81     0.1343    94.0   \n",
              " ...        ...   ...         ...   ...     ...   ...        ...     ...   \n",
              " 10943  11284.0   9.0         1.0  0.46  0.4545  0.88     0.0896    30.0   \n",
              " 10944  11964.0  17.0         1.0  0.66  0.6212  0.34     0.1343   124.0   \n",
              " 10945   5390.0  12.0         1.0  0.80  0.7273  0.43     0.2836    26.0   \n",
              " 10946    860.0   7.0         1.0  0.24  0.1970  0.65     0.4179     3.0   \n",
              " 10947  15795.0   8.0         2.0  0.52  0.5000  0.83     0.1642    33.0   \n",
              " \n",
              "          cnt      dteday  \n",
              " 0      152.0  2011-05-13  \n",
              " 1      177.0  2012-03-24  \n",
              " 2       53.0  2011-04-02  \n",
              " 3      133.0  2012-10-04  \n",
              " 4      212.0  2011-04-09  \n",
              " ...      ...         ...  \n",
              " 10943  359.0  2012-04-20  \n",
              " 10944  812.0  2012-05-18  \n",
              " 10945  189.0  2011-08-17  \n",
              " 10946  100.0  2011-02-08  \n",
              " 10947  779.0  2012-10-25  \n",
              " \n",
              " [10948 rows x 10 columns],\n",
              " {'Raw_numericalvalues': 9,\n",
              "  'Raw_categoricalvalues': 1,\n",
              "  'Raw_columns': 10,\n",
              "  'Raw_rows': 10948,\n",
              "  'Raw_missing': 0},\n",
              " {'Imputed_numericalvalues': 9,\n",
              "  'Imputed_categoricalvalues': 1,\n",
              "  'Imputed_columns': 10,\n",
              "  'Imputed_rows': 10948,\n",
              "  'Imputed_missingvalues': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_encode(imputed_data):\n",
        "    #normalizing numerical columns using robustscalar\n",
        "    numerical_columns  = [x for x in imputed_data.columns if imputed_data[x].dtype in ['int64', 'float64']]\n",
        "    scalar = RobustScaler(quantile_range=(25,75))\n",
        "    scaled = scalar.fit_transform(imputed_data[numerical_columns])\n",
        "    scaled = pd.DataFrame(scaled)\n",
        "    scaled.columns = imputed_data[numerical_columns].columns\n",
        "    \n",
        "    #dropping cat columns with more than 10 categories\n",
        "    cat_cols = [x for x in imputed_data.columns if x not in numerical_columns]\n",
        "    cat_cols_to_drop = []\n",
        "    for col in cat_cols:\n",
        "        if imputed_data[col].value_counts().count()>10:\n",
        "            cat_cols_to_drop.append(col)\n",
        "    data_for_enc = imputed_data.drop(numerical_columns,axis=1)\n",
        "    data_for_enc.drop(cat_cols_to_drop,axis=1,inplace=True)\n",
        "\n",
        "    #encoding categorical varialbles\n",
        "    try:\n",
        "        enc_data= pd.get_dummies(data_for_enc, columns=data_for_enc.columns)\n",
        "        encoded_data = pd.concat([scaled, enc_data], axis=1)\n",
        "    except:\n",
        "      encoded_data = scaled.copy()\n",
        "\n",
        "    return encoded_data"
      ],
      "metadata": {
        "id": "dNEQcU_gtEMt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = normalize_and_encode(ll[0])"
      ],
      "metadata": {
        "id": "JPk6JpBytHzU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [\n",
        "    XGBClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    GradientBoostingClassifier(),\n",
        "    LogisticRegression(),\n",
        "    DecisionTreeClassifier()\n",
        "    ]"
      ],
      "metadata": {
        "id": "lHBs8gDwZTqs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_models = [\n",
        "    KNeighborsRegressor(),\n",
        "    LinearRegression(),\n",
        "    GradientBoostingRegressor(),\n",
        "    ExtraTreesRegressor(),\n",
        "    RandomForestRegressor(),\n",
        "    DecisionTreeRegressor(),\n",
        "    Lasso(),\n",
        "    Ridge()\n",
        "]"
      ],
      "metadata": {
        "id": "qtxnpR4jwZJB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'/content/automl-bigdata-514a5baac622.json' \n",
        "def cloud_access(bucket_name, blob_name, pickle_file):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename(pickle_file)"
      ],
      "metadata": {
        "id": "S2dsziEgPMds"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def connection():\n",
        "  cred = credentials.Certificate('/content/auto-ml-af39c-firebase-adminsdk-37cmd-35f3911f5e.json')\n",
        "  try:\n",
        "    app = firebase_admin.initialize_app(cred)\n",
        "  except:\n",
        "    app = firebase_admin.initialize_app(cred, name = str(random.random()))\n",
        "  return firestore.client()\n",
        "\n",
        "def regression(train_data, y, reg_models):\n",
        "  db = connection()  \n",
        "  y_class = train_data[[y]]\n",
        "  \n",
        "  X_train, X_val, y_train, y_val = train_test_split(train_data.drop(y, axis=1), y_class, test_size=0.2, random_state=100)\n",
        "  \n",
        "  res = {}\n",
        "  \n",
        "  KNeighborsRegressor_grid = {\n",
        "      'n_neighbors':[2,5,10], \n",
        "      'weights': ['uniform', 'distance'], \n",
        "      'algorithm': ['auto','ball_tree','kd_tree','brute'],\n",
        "      'leaf_size': [15,30,45],\n",
        "      }\n",
        "\n",
        "  GradientBoostingRegressor_grid = {\n",
        "      'loss':['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
        "      'learning_rate':[0.1,0.5,0.8],\n",
        "      'n_estimators':[10,50,100]\n",
        "  }\n",
        "\n",
        "  ExtraTreesRegressor_grid = {\n",
        "      'n_estimators':[10,50,100],\n",
        "      'criterion':['squared_error', 'absolute_error', 'friedman_mse', 'poisson']\n",
        "  }\n",
        "\n",
        "  RandomForestRegressor_grid = {\n",
        "      'n_estimators':[10,50,100],\n",
        "      'criterion':['squared_error', 'absolute_error', 'friedman_mse', 'poisson']\n",
        "  }\n",
        "\n",
        "  DecisionTreeRegressor_grid = {\n",
        "      'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
        "      'splitter':['best','random']\n",
        "  }\n",
        "\n",
        "  LinearRegression_grid = {\n",
        "    'fit_intercept': [True, False]\n",
        "  }\n",
        "\n",
        "  Lasso_grid = {\n",
        "      'alpha': [0.1, 0.2, 0.5],\n",
        "      'fit_intercept': [True, False]\n",
        "  }\n",
        "  Ridge_grid = {\n",
        "       'alpha': [0.1, 0.2, 0.5],\n",
        "      'fit_intercept': [True, False]\n",
        "  }\n",
        "  \n",
        " \n",
        "  params = { \n",
        "      'KNeighborsRegressor': KNeighborsRegressor_grid,\n",
        "      'GradientBoostingRegressor': GradientBoostingRegressor_grid,\n",
        "      'ExtraTreesRegressor': ExtraTreesRegressor_grid,\n",
        "      'RandomForestRegressor': RandomForestRegressor_grid,\n",
        "      'DecisionTreeRegressor': DecisionTreeRegressor_grid,\n",
        "      'LinearRegression': LinearRegression_grid, \n",
        "      'Lasso': Lasso_grid,\n",
        "      'Ridge':Ridge_grid\n",
        "    }\n",
        "\n",
        "  clf = {}\n",
        "\n",
        "  for reg in reg_models:\n",
        "    name = reg.__class__.__name__  \n",
        "    try:\n",
        "      clf[name] = RandomizedSearchCV(reg, params[name], random_state=0)\n",
        "    except:\n",
        "      print(name)\n",
        "      continue\n",
        "    results = clf[name].fit(X_train, y_train)\n",
        "    print(results.best_params_)\n",
        "    r2 = round(r2_score(y_val, clf[name].predict(X_val)), 3)\n",
        "    rmse = round(mean_squared_error(y_val, clf[name].predict(X_val)), 3)\n",
        "    N = 16\n",
        " \n",
        "    string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    while string_name in db.collection(u'models').stream():\n",
        "        string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    print(\"{} trained with an RMSE of : {} and an accuracy of: {}\".format(name, rmse, r2))\n",
        "    \n",
        "    res[name] = {\n",
        "        'RMSE': rmse,\n",
        "         'r2': r2,\n",
        "         'params': results.best_params_\n",
        "      }  \n",
        "\n",
        "  rmse_list = []\n",
        "  r2_list = []\n",
        "  names = list(res.keys())\n",
        "  for name in res:\n",
        "    rmse_list.append(res[name]['RMSE'])\n",
        "    r2_list.append(res[name]['r2'])\n",
        "\n",
        "  if rmse_list.count(min(rmse_list)) > 1:\n",
        "    best_model = names[r2_list.index(max(r2_list))]\n",
        "  else:\n",
        "    best_model = names[rmse_list.index(min(rmse_list))]\n",
        "\n",
        "  print(best_model, clf[best_model].get_params())\n",
        "  pickle.dump(clf[best_model], open('model.pkl', 'wb'))\n",
        "  cloud_access('automl-bigdataarch', 'regression_models/model.pkl', 'model.pkl')\n",
        "  db.collection(u'models').document(string_name).set(res)\n",
        "  return best_model"
      ],
      "metadata": {
        "id": "vbFeRRK_T9zU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def connection():\n",
        "  cred = credentials.Certificate('/content/auto-ml-af39c-firebase-adminsdk-37cmd-35f3911f5e.json')\n",
        "  try:\n",
        "    app = firebase_admin.initialize_app(cred)\n",
        "  except:\n",
        "    app = firebase_admin.initialize_app(cred, name = str(random.random()))\n",
        "  return firestore.client()\n",
        "\n",
        "def classification(train_data, y, classifiers):\n",
        "  db = connection()  \n",
        "  y_class = train_data[[y]]\n",
        "  X_train, X_val, y_train, y_val = train_test_split(train_data.drop(y, axis=1), y_class, test_size=0.2, random_state=100)\n",
        "\n",
        "  res = {}\n",
        "  \n",
        "  XGBClassifier_grid = {\n",
        "      'n_estimators': stats.randint(50, 100),\n",
        "      'learning_rate': stats.uniform(0.01, 0.59),\n",
        "      'subsample': stats.uniform(0.3, 0.6),\n",
        "      'max_depth': [3, 4, 5],\n",
        "      'colsample_bytree': stats.uniform(0.5, 0.4),\n",
        "      'min_child_weight': [1, 2, 3, 4]\n",
        "      }\n",
        "\n",
        "  RandomForestClassifier_grid = {\n",
        "      'n_estimators':[10,50,100],\n",
        "      'criterion':['gini', 'entropy', 'log_loss']\n",
        "  }\n",
        "\n",
        "  GradientBoostingClassifier_grid = {\n",
        "      'loss':['log_loss', 'deviance', 'exponential'],\n",
        "      'learning_rate':[0.1,0.5]\n",
        "        }\n",
        "\n",
        "  LogisticRegression_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'dual':[True, False],\n",
        "    'fit_intercept':[True,False]\n",
        "  }\n",
        "\n",
        "  DecisionTreeClassifier_grid = {\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "    'splitter':['best', 'random']\n",
        "  }\n",
        "  \n",
        "  params = { \n",
        "      'XGBClassifier': XGBClassifier_grid,\n",
        "      'RandomForestClassifier': RandomForestClassifier_grid,\n",
        "      'GradientBoostingClassifier': GradientBoostingClassifier_grid,\n",
        "      'LogisticRegression': LogisticRegression_grid,\n",
        "      'DecisionTreeClassifier':DecisionTreeClassifier_grid\n",
        "    }\n",
        "    \n",
        "  clf = {}\n",
        "  \n",
        "  for clf1 in classifiers:\n",
        "    name = clf1.__class__.__name__\n",
        "    try:\n",
        "      clf[name] = RandomizedSearchCV(clf1, params[name], random_state=0)\n",
        "    except:\n",
        "        print(name)\n",
        "        continue \n",
        "\n",
        "    results = clf[name].fit(X_train, y_train)\n",
        "    print(results.best_params_)        \n",
        "    acc = round(balanced_accuracy_score(y_val, clf[name].predict(X_val)), 3)\n",
        "    f1 = round(f1_score(y_true=y_val, y_pred = clf[name].predict(X_val), average='weighted'), 3)\n",
        "\n",
        "    N = 16\n",
        " \n",
        "    string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    while string_name in db.collection(u'models').stream():\n",
        "        string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    print(\"{} trained with an F1 of : {} and an accuracy of: {}\".format(name, f1, acc))\n",
        "\n",
        "    res[name] = {\n",
        "        'Accuracy': acc,\n",
        "         'F1Score': f1,\n",
        "         'params': results.best_params_\n",
        "      }  \n",
        "\n",
        "  acc_list = []\n",
        "  f1_list = []\n",
        "  names = list(res.keys())\n",
        "  for name in res:\n",
        "    acc_list.append(res[name]['Accuracy'])\n",
        "    f1_list.append(res[name]['F1Score'])\n",
        "\n",
        "  if acc_list.count(max(acc_list)) > 1:\n",
        "    best_model = names[f1_list.index(max(f1_list))]\n",
        "  else:\n",
        "    best_model = names[acc_list.index(max(acc_list))]\n",
        "\n",
        "  print(best_model, clf[best_model].get_params())\n",
        "  pickle.dump(clf[best_model], open('model.pkl', 'wb'))\n",
        "  cloud_access('automl-bigdataarch', 'classification_models/model.pkl', 'model.pkl')\n",
        "  db.collection(u'models').document(string_name).set(res)\n",
        "  return best_model"
      ],
      "metadata": {
        "id": "voDTtw_pZ3XZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M9MzjE8q4eic"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression(train,'cnt',reg_models)"
      ],
      "metadata": {
        "id": "EDNRfUKvf5uM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "37423124-ba66-4d26-e174-91e06ab114b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'weights': 'distance', 'n_neighbors': 10, 'leaf_size': 15, 'algorithm': 'brute'}\n",
            "KNeighborsRegressor trained with an RMSE of : 0.164 and an accuracy of: 0.693\n",
            "{'fit_intercept': True}\n",
            "LinearRegression trained with an RMSE of : 0.244 and an accuracy of: 0.541\n",
            "{'n_estimators': 100, 'loss': 'huber', 'learning_rate': 0.5}\n",
            "GradientBoostingRegressor trained with an RMSE of : 0.078 and an accuracy of: 0.853\n",
            "{'n_estimators': 100, 'criterion': 'squared_error'}\n",
            "ExtraTreesRegressor trained with an RMSE of : 0.08 and an accuracy of: 0.849\n",
            "{'n_estimators': 100, 'criterion': 'friedman_mse'}\n",
            "RandomForestRegressor trained with an RMSE of : 0.081 and an accuracy of: 0.848\n",
            "{'splitter': 'best', 'criterion': 'squared_error'}\n",
            "DecisionTreeRegressor trained with an RMSE of : 0.156 and an accuracy of: 0.707\n",
            "{'fit_intercept': False, 'alpha': 0.1}\n",
            "Lasso trained with an RMSE of : 0.293 and an accuracy of: 0.45\n",
            "{'fit_intercept': True, 'alpha': 0.5}\n",
            "Ridge trained with an RMSE of : 0.244 and an accuracy of: 0.541\n",
            "GradientBoostingRegressor {'cv': None, 'error_score': nan, 'estimator__alpha': 0.9, 'estimator__ccp_alpha': 0.0, 'estimator__criterion': 'friedman_mse', 'estimator__init': None, 'estimator__learning_rate': 0.1, 'estimator__loss': 'squared_error', 'estimator__max_depth': 3, 'estimator__max_features': None, 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__n_estimators': 100, 'estimator__n_iter_no_change': None, 'estimator__random_state': None, 'estimator__subsample': 1.0, 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': GradientBoostingRegressor(), 'n_iter': 10, 'n_jobs': None, 'param_distributions': {'loss': ['squared_error', 'absolute_error', 'huber', 'quantile'], 'learning_rate': [0.1, 0.5, 0.8], 'n_estimators': [10, 50, 100]}, 'pre_dispatch': '2*n_jobs', 'random_state': 0, 'refit': True, 'return_train_score': False, 'scoring': None, 'verbose': 0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GradientBoostingRegressor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification(train,'Survived',classifiers)"
      ],
      "metadata": {
        "id": "l7xpgUV9brG5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}