{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "UHXIXZWD8q0n"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import string\n",
        "import random\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "import pickle\n",
        "from flask import Flask\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
        "from scipy import stats\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_df = pd.read_csv(r\"train.csv\")"
      ],
      "metadata": {
        "id": "73lSYjzI-LsZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/maximal-record-384001-406302dda581.json' "
      ],
      "metadata": {
        "id": "T6MGDyX09L5V"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "def write_read(bucket_name, blob_name):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename('/content/Dummy Data HSS.csv')"
      ],
      "metadata": {
        "id": "FdqPX3pj8qNf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "def write_read(bucket_name, blob_name):\n",
        "    \"\"\"Write and read a blob from GCS using file-like IO\"\"\"\n",
        "    # The ID of your GCS bucket\n",
        "    # bucket_name = \"your-bucket-name\"\n",
        "\n",
        "    # The ID of your new GCS object\n",
        "    # blob_name = \"storage-object-name\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "\n",
        "    # Mode can be specified as wb/rb for bytes mode.\n",
        "    # See: https://docs.python.org/3/library/io.html\n",
        "    with blob.open(\"w\") as f:\n",
        "        f.write(\"Hello world\")\n",
        "\n",
        "    with blob.open(\"r\") as f:\n",
        "        print(f.read())\n"
      ],
      "metadata": {
        "id": "XUNrTluh57vy"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute(data):\n",
        "    #checking missing values\n",
        "    percent_missing = data.isnull().sum() * 100 / data.shape[0]\n",
        "    #dropping columns if missing percentage is more than 30\n",
        "    for i in range(len(data.columns)):\n",
        "        if percent_missing[i] >30:\n",
        "            data.drop(data.columns[i],axis=1,inplace=True)\n",
        "    #getting numerical and categorical variables\n",
        "    numerical_columns = [x for x in data.columns if data[x].dtype != 'object']\n",
        "    data_num = data[numerical_columns]\n",
        "    \n",
        "    cat_columns = [x for x in data.columns if x not in numerical_columns]\n",
        "    data_cat = data[cat_columns]\n",
        "    \n",
        "    #Imputing using KNN Imputer for numerical columns\n",
        "    imputer = KNNImputer(n_neighbors=2)\n",
        "    imputed_num = imputer.fit_transform(data_num)\n",
        "    imputed_num = pd.DataFrame(imputed_num)\n",
        "    imputed_num.columns=data_num.columns\n",
        "    \n",
        "    # most frequent imputation for categorical columns\n",
        "    data_cat_imputed = data_cat.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
        "    \n",
        "    #concat the imputed dfs\n",
        "    imputed_data = pd.concat([imputed_num, data_cat_imputed], axis=1)\n",
        "    \n",
        "    #return imputed_data\n",
        "    return imputed_data"
      ],
      "metadata": {
        "id": "Gf30A0ccsoXa"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ll = impute(demo_df)"
      ],
      "metadata": {
        "id": "8nu5aEZSspik"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_encode(imputed_data):\n",
        "    #normalizing numerical columns using robustscalar\n",
        "    numerical_columns  = [x for x in imputed_data.columns if imputed_data[x].dtype in ['int64', 'float64']]\n",
        "    scalar = RobustScaler(quantile_range=(25,75))\n",
        "    scaled = scalar.fit_transform(imputed_data[numerical_columns])\n",
        "    scaled = pd.DataFrame(scaled)\n",
        "    scaled.columns = imputed_data[numerical_columns].columns\n",
        "    \n",
        "    #dropping cat columns with more than 10 categories\n",
        "    cat_cols = [x for x in imputed_data.columns if x not in numerical_columns]\n",
        "    cat_cols_to_drop = []\n",
        "    for col in cat_cols:\n",
        "        if imputed_data[col].value_counts().count()>10:\n",
        "            cat_cols_to_drop.append(col)\n",
        "    data_for_enc = imputed_data.drop(numerical_columns,axis=1)\n",
        "    data_for_enc.drop(cat_cols_to_drop,axis=1,inplace=True)\n",
        "\n",
        "    #encoding categorical varialbles\n",
        "    try:\n",
        "        enc_data= pd.get_dummies(data_for_enc, columns=data_for_enc.columns)\n",
        "        encoded_data = pd.concat([scaled, enc_data], axis=1)\n",
        "    except:\n",
        "      encoded_data = scaled.copy()\n",
        "\n",
        "    return encoded_data"
      ],
      "metadata": {
        "id": "dNEQcU_gtEMt"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = normalize_and_encode(ll)"
      ],
      "metadata": {
        "id": "JPk6JpBytHzU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [\n",
        "    XGBClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    GradientBoostingClassifier(),\n",
        "    LogisticRegression(),\n",
        "    DecisionTreeClassifier()\n",
        "    ]"
      ],
      "metadata": {
        "id": "lHBs8gDwZTqs"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_models = [\n",
        "    KNeighborsRegressor(),\n",
        "    LinearRegression(),\n",
        "    GradientBoostingRegressor(),\n",
        "    ExtraTreesRegressor(),\n",
        "    RandomForestRegressor(),\n",
        "    DecisionTreeRegressor(),\n",
        "    Lasso(),\n",
        "    Ridge()\n",
        "]"
      ],
      "metadata": {
        "id": "qtxnpR4jwZJB"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'/content/maximal-record-384001-406302dda581.json' \n",
        "def cloud_access(bucket_name, blob_name, pickle_file):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename(pickle_file)"
      ],
      "metadata": {
        "id": "S2dsziEgPMds"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def connection():\n",
        "  cred = credentials.Certificate('/content/auto-ml-af39c-firebase-adminsdk-37cmd-35f3911f5e.json')\n",
        "  try:\n",
        "    app = firebase_admin.initialize_app(cred)\n",
        "  except:\n",
        "    app = firebase_admin.initialize_app(cred, name = str(random.random()))\n",
        "  return firestore.client()\n",
        "\n",
        "def regression(train_data, y, reg_models):\n",
        "  db = connection()  \n",
        "  y_class = train_data[[y]]\n",
        "  \n",
        "  X_train, X_val, y_train, y_val = train_test_split(train_data.drop(y, axis=1), y_class, test_size=0.2, random_state=100)\n",
        "  \n",
        "  res = {}\n",
        "  \n",
        "  KNeighborsRegressor_grid = {\n",
        "      'n_neighbors':[2,5,10], \n",
        "      'weights': ['uniform', 'distance'], \n",
        "      'algorithm': ['auto','ball_tree','kd_tree','brute'],\n",
        "      'leaf_size': [15,30,45],\n",
        "      }\n",
        "\n",
        "  GradientBoostingRegressor_grid = {\n",
        "      'loss':['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
        "      'learning_rate':[0.1,0.5,0.8],\n",
        "      'n_estimators':[10,50,100]\n",
        "  }\n",
        "\n",
        "  ExtraTreesRegressor_grid = {\n",
        "      'n_estimators':[10,50,100],\n",
        "      'criterion':['squared_error', 'absolute_error', 'friedman_mse', 'poisson']\n",
        "  }\n",
        "\n",
        "  RandomForestRegressor_grid = {\n",
        "      'n_estimators':[10,50,100],\n",
        "      'criterion':['squared_error', 'absolute_error', 'friedman_mse', 'poisson']\n",
        "  }\n",
        "\n",
        "  DecisionTreeRegressor_grid = {\n",
        "      'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
        "      'splitter':['best','random']\n",
        "  }\n",
        "\n",
        "  LinearRegression_grid = {\n",
        "    'fit_intercept': [True, False]\n",
        "  }\n",
        "\n",
        "  Lasso_grid = {\n",
        "      'alpha': [0.1, 0.2, 0.5],\n",
        "      'fit_intercept': [True, False]\n",
        "  }\n",
        "  Ridge_grid = {\n",
        "       'alpha': [0.1, 0.2, 0.5],\n",
        "      'fit_intercept': [True, False]\n",
        "  }\n",
        "  \n",
        " \n",
        "  params = { \n",
        "      'KNeighborsRegressor': KNeighborsRegressor_grid,\n",
        "      'GradientBoostingRegressor': GradientBoostingRegressor_grid,\n",
        "      'ExtraTreesRegressor': ExtraTreesRegressor_grid,\n",
        "      'RandomForestRegressor': RandomForestRegressor_grid,\n",
        "      'DecisionTreeRegressor': DecisionTreeRegressor_grid,\n",
        "      'LinearRegression': LinearRegression_grid, \n",
        "      'Lasso': Lasso_grid,\n",
        "      'Ridge':Ridge_grid\n",
        "    }\n",
        "\n",
        "  clf = {}\n",
        "\n",
        "  for reg in reg_models:\n",
        "    name = reg.__class__.__name__  \n",
        "    try:\n",
        "      clf[name] = RandomizedSearchCV(reg, params[name], random_state=0)\n",
        "    except:\n",
        "      print(name)\n",
        "      continue\n",
        "    results = clf[name].fit(X_train, y_train)\n",
        "    print(results.best_params_)\n",
        "    r2 = round(r2_score(y_val, clf[name].predict(X_val)), 3)\n",
        "    rmse = round(mean_squared_error(y_val, clf[name].predict(X_val)), 3)\n",
        "    N = 16\n",
        " \n",
        "    string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    while string_name in db.collection(u'models').stream():\n",
        "        string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    print(\"{} trained with an RMSE of : {} and an accuracy of: {}\".format(name, rmse, r2))\n",
        "    \n",
        "    res[name] = {\n",
        "        'RMSE': rmse,\n",
        "         'r2': r2,\n",
        "         'params': results.best_params_\n",
        "      }  \n",
        "\n",
        "  rmse_list = []\n",
        "  r2_list = []\n",
        "  names = list(res.keys())\n",
        "  for name in res:\n",
        "    rmse_list.append(res[name]['RMSE'])\n",
        "    r2_list.append(res[name]['r2'])\n",
        "\n",
        "  if rmse_list.count(min(rmse_list)) > 1:\n",
        "    best_model = names[r2_list.index(max(r2_list))]\n",
        "  else:\n",
        "    best_model = names[rmse_list.index(min(rmse_list))]\n",
        "\n",
        "  print(best_model, clf[best_model].get_params())\n",
        "  pickle.dump(clf[best_model], open('model.pkl', 'wb'))\n",
        "  cloud_access('automl-bigdata', 'model.pkl', 'model.pkl')\n",
        "  db.collection(u'models').document(string_name).set(res)\n",
        "  return best_model"
      ],
      "metadata": {
        "id": "vbFeRRK_T9zU"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def connection():\n",
        "  cred = credentials.Certificate('/content/auto-ml-af39c-firebase-adminsdk-37cmd-35f3911f5e.json')\n",
        "  try:\n",
        "    app = firebase_admin.initialize_app(cred)\n",
        "  except:\n",
        "    app = firebase_admin.initialize_app(cred, name = str(random.random()))\n",
        "  return firestore.client()\n",
        "\n",
        "def classification(train_data, y, classifiers):\n",
        "  db = connection()  \n",
        "  y_class = train_data[[y]]\n",
        "  X_train, X_val, y_train, y_val = train_test_split(train_data.drop(y, axis=1), y_class, test_size=0.2, random_state=100)\n",
        "\n",
        "  res = {}\n",
        "  \n",
        "  XGBClassifier_grid = {\n",
        "      'n_estimators': stats.randint(50, 100),\n",
        "      'learning_rate': stats.uniform(0.01, 0.59),\n",
        "      'subsample': stats.uniform(0.3, 0.6),\n",
        "      'max_depth': [3, 4, 5],\n",
        "      'colsample_bytree': stats.uniform(0.5, 0.4),\n",
        "      'min_child_weight': [1, 2, 3, 4]\n",
        "      }\n",
        "\n",
        "  RandomForestClassifier_grid = {\n",
        "      'n_estimators':[10,50,100],\n",
        "      'criterion':['gini', 'entropy', 'log_loss']\n",
        "  }\n",
        "\n",
        "  GradientBoostingClassifier_grid = {\n",
        "      'loss':['log_loss', 'deviance', 'exponential'],\n",
        "      'learning_rate':[0.1,0.5]\n",
        "        }\n",
        "\n",
        "  LogisticRegression_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'dual':[True, False],\n",
        "    'fit_intercept':[True,False]\n",
        "  }\n",
        "\n",
        "  DecisionTreeClassifier_grid = {\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "    'splitter':['best', 'random']\n",
        "  }\n",
        "  \n",
        "  params = { \n",
        "      'XGBClassifier': XGBClassifier_grid,\n",
        "      'RandomForestClassifier': RandomForestClassifier_grid,\n",
        "      'GradientBoostingClassifier': GradientBoostingClassifier_grid,\n",
        "      'LogisticRegression': LogisticRegression_grid,\n",
        "      'DecisionTreeClassifier':DecisionTreeClassifier_grid\n",
        "    }\n",
        "    \n",
        "  clf = {}\n",
        "  \n",
        "  for clf1 in classifiers:\n",
        "    name = clf1.__class__.__name__\n",
        "    try:\n",
        "      clf[name] = RandomizedSearchCV(clf1, params[name], random_state=0)\n",
        "    except:\n",
        "        print(name)\n",
        "        continue \n",
        "\n",
        "    results = clf[name].fit(X_train, y_train)\n",
        "    print(results.best_params_)        \n",
        "    acc = round(balanced_accuracy_score(y_val, clf[name].predict(X_val)), 3)\n",
        "    f1 = round(f1_score(y_true=y_val, y_pred = clf[name].predict(X_val), average='weighted'), 3)\n",
        "\n",
        "    N = 16\n",
        " \n",
        "    string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    while string_name in db.collection(u'models').stream():\n",
        "        string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    print(\"{} trained with an F1 of : {} and an accuracy of: {}\".format(name, f1, acc))\n",
        "\n",
        "    res[name] = {\n",
        "        'Accuracy': acc,\n",
        "         'F1Score': f1,\n",
        "         'params': results.best_params_\n",
        "      }  \n",
        "\n",
        "  acc_list = []\n",
        "  f1_list = []\n",
        "  names = list(res.keys())\n",
        "  for name in res:\n",
        "    acc_list.append(res[name]['Accuracy'])\n",
        "    f1_list.append(res[name]['F1Score'])\n",
        "\n",
        "  if acc_list.count(max(acc_list)) > 1:\n",
        "    best_model = names[f1_list.index(max(f1_list))]\n",
        "  else:\n",
        "    best_model = names[acc_list.index(max(acc_list))]\n",
        "\n",
        "  print(best_model, clf[best_model].get_params())\n",
        "  pickle.dump(clf[best_model], open('model.pkl', 'wb'))\n",
        "  cloud_access('automl-bigdata', 'model.pkl', 'model.pkl')\n",
        "  db.collection(u'models').document(string_name).set(res)\n",
        "  return best_model"
      ],
      "metadata": {
        "id": "voDTtw_pZ3XZ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M9MzjE8q4eic"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression(train,'cnt',reg_models)"
      ],
      "metadata": {
        "id": "EDNRfUKvf5uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification(train,'Survived',classifiers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "l7xpgUV9brG5",
        "outputId": "8a4c4f20-c714-4f9f-960e-c5243f2f194d"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'colsample_bytree': 0.7470541988303508, 'learning_rate': 0.3711364764062286, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 65, 'subsample': 0.8415590853176427}\n",
            "XGBClassifier trained with an F1 of : 0.751 and an accuracy of: 0.728\n",
            "{'n_estimators': 100, 'criterion': 'entropy'}\n",
            "RandomForestClassifier trained with an F1 of : 0.79 and an accuracy of: 0.768\n",
            "{'loss': 'log_loss', 'learning_rate': 0.1}\n",
            "GradientBoostingClassifier trained with an F1 of : 0.791 and an accuracy of: 0.774\n",
            "{'penalty': 'l2', 'fit_intercept': True, 'dual': False}\n",
            "LogisticRegression trained with an F1 of : 0.79 and an accuracy of: 0.768\n",
            "{'splitter': 'best', 'criterion': 'entropy'}\n",
            "DecisionTreeClassifier trained with an F1 of : 0.725 and an accuracy of: 0.712\n",
            "GradientBoostingClassifier {'cv': None, 'error_score': nan, 'estimator__ccp_alpha': 0.0, 'estimator__criterion': 'friedman_mse', 'estimator__init': None, 'estimator__learning_rate': 0.1, 'estimator__loss': 'log_loss', 'estimator__max_depth': 3, 'estimator__max_features': None, 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__n_estimators': 100, 'estimator__n_iter_no_change': None, 'estimator__random_state': None, 'estimator__subsample': 1.0, 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': GradientBoostingClassifier(), 'n_iter': 10, 'n_jobs': None, 'param_distributions': {'loss': ['log_loss', 'deviance', 'exponential'], 'learning_rate': [0.1, 0.5]}, 'pre_dispatch': '2*n_jobs', 'random_state': 0, 'refit': True, 'return_train_score': False, 'scoring': None, 'verbose': 0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GradientBoostingClassifier'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-K22mvlgIB0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}