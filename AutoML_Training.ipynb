{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UHXIXZWD8q0n"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import string\n",
        "import random\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_df = pd.read_csv(r\"train.csv\")"
      ],
      "metadata": {
        "id": "73lSYjzI-LsZ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/maximal-record-384001-406302dda581.json' "
      ],
      "metadata": {
        "id": "T6MGDyX09L5V"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "def write_read(bucket_name, blob_name):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename('/content/Dummy Data HSS.csv')"
      ],
      "metadata": {
        "id": "FdqPX3pj8qNf"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "def write_read(bucket_name, blob_name):\n",
        "    \"\"\"Write and read a blob from GCS using file-like IO\"\"\"\n",
        "    # The ID of your GCS bucket\n",
        "    # bucket_name = \"your-bucket-name\"\n",
        "\n",
        "    # The ID of your new GCS object\n",
        "    # blob_name = \"storage-object-name\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "\n",
        "    # Mode can be specified as wb/rb for bytes mode.\n",
        "    # See: https://docs.python.org/3/library/io.html\n",
        "    with blob.open(\"w\") as f:\n",
        "        f.write(\"Hello world\")\n",
        "\n",
        "    with blob.open(\"r\") as f:\n",
        "        print(f.read())\n"
      ],
      "metadata": {
        "id": "XUNrTluh57vy"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute(data):\n",
        "    #checking missing values\n",
        "    percent_missing = data.isnull().sum() * 100 / data.shape[0]\n",
        "    #dropping columns if missing percentage is more than 30\n",
        "    for i in range(len(data.columns)):\n",
        "        if percent_missing[i] >30:\n",
        "            data.drop(data.columns[i],axis=1,inplace=True)\n",
        "    #getting numerical and categorical variables\n",
        "    numerical_columns = [x for x in data.columns if data[x].dtype != 'object']\n",
        "    data_num = data[numerical_columns]\n",
        "    \n",
        "    cat_columns = [x for x in data.columns if x not in numerical_columns]\n",
        "    data_cat = data[cat_columns]\n",
        "    \n",
        "    #Imputing using KNN Imputer for numerical columns\n",
        "    imputer = KNNImputer(n_neighbors=2)\n",
        "    imputed_num = imputer.fit_transform(data_num)\n",
        "    imputed_num = pd.DataFrame(imputed_num)\n",
        "    imputed_num.columns=data_num.columns\n",
        "    \n",
        "    # most frequent imputation for categorical columns\n",
        "    data_cat_imputed = data_cat.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
        "    \n",
        "    #concat the imputed dfs\n",
        "    imputed_data = pd.concat([imputed_num, data_cat_imputed], axis=1)\n",
        "    \n",
        "    #return imputed_data\n",
        "    return imputed_data"
      ],
      "metadata": {
        "id": "Gf30A0ccsoXa"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ll = impute(demo_df)"
      ],
      "metadata": {
        "id": "8nu5aEZSspik"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_encode(imputed_data):\n",
        "    #normalizing numerical columns using robustscalar\n",
        "    numerical_columns  = [x for x in imputed_data.columns if imputed_data[x].dtype in ['int64', 'float64']]\n",
        "    scalar = RobustScaler(quantile_range=(25,75))\n",
        "    scaled = scalar.fit_transform(imputed_data[numerical_columns])\n",
        "    scaled = pd.DataFrame(scaled)\n",
        "    scaled.columns = imputed_data[numerical_columns].columns\n",
        "    \n",
        "    #dropping cat columns with more than 10 categories\n",
        "    cat_cols = [x for x in imputed_data.columns if x not in numerical_columns]\n",
        "    cat_cols_to_drop = []\n",
        "    for col in cat_cols:\n",
        "        if imputed_data[col].value_counts().count()>10:\n",
        "            cat_cols_to_drop.append(col)\n",
        "    data_for_enc = imputed_data.drop(numerical_columns,axis=1)\n",
        "    data_for_enc.drop(cat_cols_to_drop,axis=1,inplace=True)\n",
        "\n",
        "    #encoding categorical varialbles\n",
        "    try:\n",
        "        enc_data= pd.get_dummies(data_for_enc, columns=data_for_enc.columns)\n",
        "        encoded_data = pd.concat([scaled, enc_data], axis=1)\n",
        "    except:\n",
        "      encoded_data = scaled.copy()\n",
        "\n",
        "    return encoded_data"
      ],
      "metadata": {
        "id": "dNEQcU_gtEMt"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = normalize_and_encode(ll)"
      ],
      "metadata": {
        "id": "JPk6JpBytHzU"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_models = [\n",
        "    KNeighborsRegressor(),\n",
        "    LinearRegression(),\n",
        "    GradientBoostingRegressor(),\n",
        "    ExtraTreesRegressor(),\n",
        "    RandomForestRegressor(),\n",
        "    DecisionTreeRegressor(),\n",
        "    Lasso(),\n",
        "    Ridge()\n",
        "]"
      ],
      "metadata": {
        "id": "qtxnpR4jwZJB"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'/content/maximal-record-384001-406302dda581.json' \n",
        "def cloud_access(bucket_name, blob_name, pickle_file):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename(pickle_file)"
      ],
      "metadata": {
        "id": "S2dsziEgPMds"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def connection():\n",
        "  cred = credentials.Certificate('/content/auto-ml-af39c-firebase-adminsdk-37cmd-35f3911f5e.json')\n",
        "  try:\n",
        "    app = firebase_admin.initialize_app(cred)\n",
        "  except:\n",
        "    app = firebase_admin.initialize_app(cred, name = str(random.random()))\n",
        "  return firestore.client()\n",
        "\n",
        "def training(train_data, y, reg_models):\n",
        "  db = connection()  \n",
        "  y_class = train_data[[y]]\n",
        "  \n",
        "  X_train, X_val, y_train, y_val = train_test_split(train_data.drop(y, axis=1), y_class, test_size=0.2, random_state=100)\n",
        "  \n",
        "  res = {}\n",
        "  \n",
        "  KNeighborsRegressor_grid = {\n",
        "      'n_neighbors':[2,5,10], \n",
        "      'weights': ['uniform', 'distance'], \n",
        "      'algorithm': ['auto','ball_tree','kd_tree','brute'],\n",
        "      'leaf_size': [15,30,45],\n",
        "      }\n",
        "\n",
        "  GradientBoostingRegressor_grid = {\n",
        "      'loss':['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
        "      'learning_rate':[0.1,0.5,0.8],\n",
        "      'n_estimators':[10,50,100]\n",
        "  }\n",
        "\n",
        "  ExtraTreesRegressor_grid = {\n",
        "      'n_estimators':[10,50,100],\n",
        "      'criterion':['squared_error', 'absolute_error', 'friedman_mse', 'poisson']\n",
        "  }\n",
        "\n",
        "  RandomForestRegressor_grid = {\n",
        "      'n_estimators':[10,50,100],\n",
        "      'criterion':['squared_error', 'absolute_error', 'friedman_mse', 'poisson']\n",
        "  }\n",
        "\n",
        "  DecisionTreeRegressor_grid = {\n",
        "      'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
        "      'splitter':['best','random']\n",
        "  }\n",
        "\n",
        "  LinearRegression_grid = {\n",
        "    'fit_intercept': [True, False]\n",
        "  }\n",
        "\n",
        "  Lasso_grid = {\n",
        "      'alpha': [0.1, 0.2, 0.5],\n",
        "      'fit_intercept': [True, False]\n",
        "  }\n",
        "  Ridge_grid = {\n",
        "       'alpha': [0.1, 0.2, 0.5],\n",
        "      'fit_intercept': [True, False]\n",
        "  }\n",
        "  \n",
        " \n",
        "  params = { \n",
        "      'KNeighborsRegressor': KNeighborsRegressor_grid,\n",
        "      'GradientBoostingRegressor': GradientBoostingRegressor_grid,\n",
        "      'ExtraTreesRegressor': ExtraTreesRegressor_grid,\n",
        "      'RandomForestRegressor': RandomForestRegressor_grid,\n",
        "      'DecisionTreeRegressor': DecisionTreeRegressor_grid,\n",
        "      'LinearRegression': LinearRegression_grid, \n",
        "      'Lasso': Lasso_grid,\n",
        "      'Ridge':Ridge_grid\n",
        "    }\n",
        "\n",
        "  clf = {}\n",
        "\n",
        "  for reg in reg_models:\n",
        "    name = reg.__class__.__name__  \n",
        "    try:\n",
        "      clf[name] = RandomizedSearchCV(reg, params[name], random_state=0)\n",
        "    except:\n",
        "      print(name)\n",
        "      continue\n",
        "    results = clf[name].fit(X_train, y_train)\n",
        "    print(results.best_params_)\n",
        "    r2 = round(r2_score(y_val, clf[name].predict(X_val)), 3)\n",
        "    rmse = round(mean_squared_error(y_val, clf[name].predict(X_val)), 3)\n",
        "    N = 16\n",
        " \n",
        "    string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    while string_name in db.collection(u'models').stream():\n",
        "        string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
        "\n",
        "    print(\"{} trained with an RMSE of : {} and an accuracy of: {}\".format(name, rmse, r2))\n",
        "    \n",
        "    res[name] = {\n",
        "        'RMSE': rmse,\n",
        "         'r2': r2,\n",
        "         'params': results.best_params_\n",
        "      }  \n",
        "\n",
        "  rmse_list = []\n",
        "  r2_list = []\n",
        "  names = list(res.keys())\n",
        "  for name in res:\n",
        "    rmse_list.append(res[name]['RMSE'])\n",
        "    r2_list.append(res[name]['r2'])\n",
        "\n",
        "  if rmse_list.count(min(rmse_list)) > 1:\n",
        "    best_model = names[r2_list.index(max(r2_list))]\n",
        "  else:\n",
        "    best_model = names[rmse_list.index(min(rmse_list))]\n",
        "\n",
        "  print(best_model, clf[best_model].get_params())\n",
        "  pickle.dump(clf[best_model], open('model.pkl', 'wb'))\n",
        "  cloud_access('automl-bigdata', 'model.pkl', 'model.pkl')\n",
        "  db.collection(u'models').document(string_name).set(res)\n",
        "  return best_model"
      ],
      "metadata": {
        "id": "vbFeRRK_T9zU"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M9MzjE8q4eic"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training(train,'cnt',reg_models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "l7xpgUV9brG5",
        "outputId": "3a799f8b-622a-4756-ce4b-8cc0999c9939"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'weights': 'distance', 'n_neighbors': 10, 'leaf_size': 15, 'algorithm': 'brute'}\n",
            "KNeighborsRegressor trained with an RMSE of : 0.164 and an accuracy of: 0.693\n",
            "{'fit_intercept': True}\n",
            "LinearRegression trained with an RMSE of : 0.244 and an accuracy of: 0.541\n",
            "{'n_estimators': 100, 'loss': 'huber', 'learning_rate': 0.5}\n",
            "GradientBoostingRegressor trained with an RMSE of : 0.077 and an accuracy of: 0.855\n",
            "{'fit_intercept': False, 'alpha': 0.1}\n",
            "Lasso trained with an RMSE of : 0.293 and an accuracy of: 0.45\n",
            "{'fit_intercept': True, 'alpha': 0.5}\n",
            "Ridge trained with an RMSE of : 0.244 and an accuracy of: 0.541\n",
            "GradientBoostingRegressor {'cv': None, 'error_score': nan, 'estimator__alpha': 0.9, 'estimator__ccp_alpha': 0.0, 'estimator__criterion': 'friedman_mse', 'estimator__init': None, 'estimator__learning_rate': 0.1, 'estimator__loss': 'squared_error', 'estimator__max_depth': 3, 'estimator__max_features': None, 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__n_estimators': 100, 'estimator__n_iter_no_change': None, 'estimator__random_state': None, 'estimator__subsample': 1.0, 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': GradientBoostingRegressor(), 'n_iter': 10, 'n_jobs': None, 'param_distributions': {'loss': ['squared_error', 'absolute_error', 'huber', 'quantile'], 'learning_rate': [0.1, 0.5, 0.8], 'n_estimators': [10, 50, 100]}, 'pre_dispatch': '2*n_jobs', 'random_state': 0, 'refit': True, 'return_train_score': False, 'scoring': None, 'verbose': 0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GradientBoostingRegressor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    }
  ]
}