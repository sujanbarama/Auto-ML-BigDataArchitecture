{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import string\n",
    "import random\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'C:\\sujan\\git learning\\git\\bigdata\\maximal-record-384001-406302dda581.json' \n",
    "def cloud_read(bucket_name, blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    data = blob.download_as_bytes()\n",
    "    df = pd.read_csv(io.BytesIO(data))\n",
    "    print(f'Pulled down file from bucket {bucket_name}, file name: {blob_name}')\n",
    "    return df\n",
    "    # with blob.open('r') as f:\n",
    "    #     csv_file = f.read()\n",
    "    # return csv_file\n",
    "\n",
    "def cloud_write(bucket_name, blob_name, csv_file):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(csv_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_encode():\n",
    "    imputed_data = cloud_read('automl-bigdata','remove-2.csv')\n",
    "    imputed_data = imputed_data.drop('Sales', axis=1)\n",
    "    #normalizing numerical columns using robustscalar\n",
    "    numerical_columns  = [x for x in imputed_data.columns if imputed_data[x].dtype in ['int64', 'float64']]\n",
    "    scalar = RobustScaler(quantile_range=(25,75))\n",
    "    scaled = scalar.fit_transform(imputed_data[numerical_columns])\n",
    "    scaled = pd.DataFrame(scaled)\n",
    "    scaled.columns = imputed_data[numerical_columns].columns\n",
    "    \n",
    "    #dropping cat columns with more than 10 categories\n",
    "    cat_cols = [x for x in imputed_data.columns if x not in numerical_columns]\n",
    "    cat_cols_to_drop = []\n",
    "    for col in cat_cols:\n",
    "        if imputed_data[col].value_counts().count()>10:\n",
    "            cat_cols_to_drop.append(col)\n",
    "    data_for_enc = imputed_data.drop(numerical_columns,axis=1)\n",
    "    data_for_enc.drop(cat_cols_to_drop,axis=1,inplace=True)\n",
    "\n",
    "    #encoding categorical varialbles\n",
    "    enc_data= pd.get_dummies(data_for_enc, columns=data_for_enc.columns)\n",
    "    \n",
    "    encoded_data = pd.concat([scaled, enc_data], axis=1)\n",
    "    encoded_data.to_csv('upload_encoded.csv',index = False)\n",
    "    cloud_write('automl-bigdata', 'encoded_data.csv','upload_encoded.csv')\n",
    "\n",
    "    return {\n",
    "        'message': 'Success',\n",
    "        # 'file': pickle.load('bvbhj.pickle')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled down file from bucket automl-bigdata, file name: remove-2.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Success'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_and_encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "  reg_models = [\n",
    "    KNeighborsRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    ExtraTreesRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    DecisionTreeRegressor(),\n",
    "    LinearRegression(),\n",
    "    Lasso(),\n",
    "    Ridge()\n",
    "]\n",
    "  train_data = cloud_read('automl-bigdata','encoded_data.csv')\n",
    "  y_class = train_data[['Sales']]\n",
    "  \n",
    "  X_train, X_val, y_train, y_val = train_test_split(train_data.drop('Sales', axis=1), y_class, test_size=0.2, random_state=100)\n",
    "  \n",
    "  res = {}\n",
    "  \n",
    "  KNeighborsRegressor_grid = {\n",
    "      'n_neighbors':[2,5,10], \n",
    "      'weights': ['uniform', 'distance'], \n",
    "      'algorithm': ['auto','ball_tree','kd_tree','brute'],\n",
    "      'leaf_size': [15,30,45],\n",
    "      }\n",
    "\n",
    "  GradientBoostingRegressor_grid = {\n",
    "      'loss':['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "      'learning_rate':[0.1,0.5,0.8],\n",
    "      'n_estimators':[10,50,100]\n",
    "  }\n",
    "\n",
    "  ExtraTreesRegressor_grid = {\n",
    "      'n_estimators':[10,50,100],\n",
    "      'criterion':['squared_error', 'absolute_error', 'friedman_mse', 'poisson']\n",
    "  }\n",
    "\n",
    "  RandomForestRegressor_grid = {\n",
    "      'n_estimators':[10,50,100],\n",
    "      'criterion':['squared_error', 'absolute_error', 'friedman_mse', 'poisson']\n",
    "  }\n",
    "\n",
    "  DecisionTreeRegressor_grid = {\n",
    "      'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "      'splitter':['best','random']\n",
    "  }\n",
    "\n",
    "  LinearRegression_grid = {\n",
    "    'fit_intercept': [True, False]\n",
    "  }\n",
    "\n",
    "  Lasso_grid = {\n",
    "      'alpha': [0.1, 0.2, 0.5],\n",
    "      'fit_intercept': [True, False]\n",
    "  }\n",
    "  Ridge_grid = {\n",
    "       'alpha': [0.1, 0.2, 0.5],\n",
    "      'fit_intercept': [True, False]\n",
    "  }\n",
    "  \n",
    " \n",
    "  params = { \n",
    "      'KNeighborsRegressor': KNeighborsRegressor_grid,\n",
    "      'GradientBoostingRegressor': GradientBoostingRegressor_grid,\n",
    "      'ExtraTreesRegressor': ExtraTreesRegressor_grid,\n",
    "      'RandomForestRegressor': RandomForestRegressor_grid,\n",
    "      'DecisionTreeRegressor': DecisionTreeRegressor_grid,\n",
    "      'LinearRegression': LinearRegression_grid, \n",
    "      'Lasso': Lasso_grid,\n",
    "      'Ridge':Ridge_grid\n",
    "    }\n",
    "\n",
    "  for reg in reg_models:\n",
    "    name = reg.__class__.__name__  \n",
    "    try:\n",
    "      clf = RandomizedSearchCV(reg, params[name], random_state=0)\n",
    "    except:\n",
    "      print(name)\n",
    "      continue\n",
    "    results = clf.fit(X_train, y_train)\n",
    "    print(results.best_params_)\n",
    "    r2 = round(r2_score(y_val, clf.predict(X_val)), 3)\n",
    "    rmse = round(mean_squared_error(y_val, clf.predict(X_val)), 3)\n",
    "    N = 16\n",
    " \n",
    "    # string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
    "\n",
    "    # while string_name in db.collection(u'models').stream():\n",
    "    #     string_name = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k = N))\n",
    "\n",
    "    print(\"{} trained with an RMSE of : {} and an accuracy of: {}\".format(name, rmse, r2))\n",
    "    \n",
    "    res[name] = {\n",
    "        'RMSE': rmse,\n",
    "         'r2': r2,\n",
    "         'params': results.best_params_\n",
    "      }  \n",
    "  return {\n",
    "        'message': 'Success',\n",
    "        # 'file': pickle.load('bvbhj.pickle')\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled down file from bucket automl-bigdata, file name: encoded_data.csv\n",
      "{'weights': 'distance', 'n_neighbors': 5, 'leaf_size': 30, 'algorithm': 'ball_tree'}\n",
      "KNeighborsRegressor trained with an RMSE of : 86.357 and an accuracy of: 0.99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Success'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
